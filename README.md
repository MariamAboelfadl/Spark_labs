# ⚡ Spark Labs

A collection of practice projects built with **Apache Spark** for learning and hands-on experience in data processing and analysis.

---

## 📂 Projects Overview

### 1️⃣ Banking Data Processing
- Practice on handling **bank datasets**.  
- Focus: cleaning, transforming, and analyzing structured financial data.

### 2️⃣ Annual Exam Results Analysis(3thanwy)
- Analyzed yearly **student results dataset**.  
- Extracted insights using **Spark SQL & DataFrames**.  

### 3️⃣ Chicago Taxi Trips
- Worked on **Chicago taxi pickup dataset**.  
- Tasks included filtering, aggregations, and identifying patterns in rides.

### 4️⃣ Criminal Records Dataset
- Processed dataset of **criminal records**.  
- Goal: explore data and transformations with Spark.

### 5️⃣ HR Data from Oracle
- Extracted **HR tables** from Oracle database.  
- Extracted HR files from kaggle as CSV.  
- Merged datasets and Applied transformations (e.g. cleaning, joins) at hr tables then uion all.  
- *Load step pending* (to be completed in next iteration).

---

## 🛠️ Tech Stack
- **Apache Spark** (PySpark)  
- **Oracle Database** (data extraction)  
- **CSV / Parquet** for storage  
- **Python & SQL** for transformations  

---

## 🚀 How to Run
1. Clone Repository:
   ```bash
   git clone https://github.com/MariamAboelfadl/Spark_labs.git
   cd MariamAboelfadl/Spark_labs
   ```
2. Install requirements:
   ```bash
   pip install -r requirements.txt
   ```
3. Run Jupyter notebooks or Spark jobs for each project.

---

## 📈 Key Learnings
- Building ET pipelines with Spark  
- Data extraction from relational databases ,csvs and excel
- Cleaning & transforming raw datasets  
- Working with **structured and semi-structured data**  

---

## 🚧 Future Enhancements
- Add **loading step** for HR data (e.g. load into PostgreSQL or BigQuery).  
- Containerize environment with **Docker**.  
- Add **Airflow orchestration** for automation.  

